# Kubernetes Components and Entities Summary

## Pod
- **Базовая единица выполнения**: Наименьшая и простейшая сущность в Kubernetes, представляющая один или несколько контейнеров.
- **Совместное использование ресурсов**: Контейнеры в одном Pod разделяют сетевое пространство (IP-адрес, порты) и хранилище (volumes).
- **Жизненный цикл**: Pod создается, работает и удаляется как единое целое. При сбое перезапускается или заменяется.
- **Использование**: Обычно управляется контроллерами (Deployment, StatefulSet и др.), а не создается напрямую.

## ReplicaSet
- **Контроллер репликации**: Обеспечивает работу заданного количества идентичных Pod-реплик в любой момент времени.
- **Селектор меток**: Определяет, какие Pods управляются. Поддерживает условия равенства и набора.
- **Масштабирование**: Позволяет увеличивать или уменьшать количество реплик.
- **Наследование**: Является основой для более высокоуровневых контроллеров, таких как Deployment.

## DaemonSet
- **Фоновая репликация**: Гарантирует, что на всех (или некоторых) узлах кластера работает копия указанного Pod.
- **Типичные сценарии**: Сбор логов (Fluentd), мониторинг (Node Exporter), сетевые плагины.
- **Автоматическое размещение**: При добавлении нового узла в кластер DaemonSet автоматически создает Pod на нем.

## Service
- **Абстракция доступа к приложениям**: Определяет логический набор Pods (через селектор меток) и политику доступа к ним.
- **Стабильная сетевая точка**: Предоставляет постоянный IP-адрес и DNS-имя, даже если Pods пересоздаются.
- **Типы**:
    - `ClusterIP`: Внутренний IP для доступа внутри кластера.
    - `NodePort`: Открывает порт на каждом узле для внешнего доступа.
    - `LoadBalancer`: Интегрируется с облачным балансировщиком нагрузки.
    - `ExternalName`: Сопоставляет сервис с внешним DNS-именем.

## Ingress
- **Маршрутизация внешнего трафика**: Управляет внешним HTTP/HTTPS доступом к сервисам внутри кластера.
- **Основа на правилах**: Определяет правила маршрутизации (хосты, пути) к соответствующим бэкенд-сервисам.
- **Требует контроллера Ingress**: Для работы необходим запущенный Ingress Controller (например, nginx, Traefik).

## Egress
- **Управление исходящим трафиком**: Политики для контроля исходящих соединений Pods с внешними ресурсами.
- **Реализация**: Часто настраивается через Network Policies или сторонние решения (например, через Service Mesh). (Примечание: В предоставленном документе Egress отдельно не описан).

## Deployment
- **Декларативное обновление приложений**: Высокоуровневый контроллер для управления состоянием приложения через Pod-шаблоны.
- **Основные функции**:
    - Развертывание и масштабирование ReplicaSet.
    - Обновление версий приложения (rolling update, rollback).
    - Управление жизненным циклом приложения.
- **Желаемое состояние**: Пользователь описывает желаемое состояние, а Deployment приводит текущее состояние к нему.

## StatefulSet
- **Управление stateful-приложениями**: Предназначен для приложений, которым требуется устойчивая идентичность и устойчивое хранилище.
- **Ключевые особенности**:
    - Стабильные, уникальные сетевые идентификаторы (имена Pods, DNS-записи).
    - Упорядоченное и предсказуемое развертывание и масштабирование.
    - Устойчивое хранилище, привязанное к каждому экземпляру Pod (через PersistentVolumeClaims).
- **Примеры использования**: Базы данных (MySQL, PostgreSQL), кластерные приложения.

## PersistentVolume (PV) и PersistentVolumeClaim (PVC)
- **Управление хранилищем**: Абстракция для предоставления хранилища Pod-ам независимо от деталей реализации.
- **PersistentVolume (PV)**:
    - Ресурс хранилища в кластере (например, диск в облаке, NFS-шара).
    - Существует независимо от жизненного цикла любого отдельного Pod.
- **PersistentVolumeClaim (PVC)**:
    - Запрос на хранение от пользователя (разработчика).
    - Определяет требуемый объем, режим доступа (RWO, ROX, RWX).
    - PVC связывается с подходящим PV.
- **Использование в Pod**: Pod ссылается на PVC в спецификации тома, что позволяет монтировать устойчивое хранилище в контейнер.

-----------
# Компоненты кластера Kubernetes, обеспечивающие работу API сервера

## **Управляющий слой (Control Plane)**

### **1. kube-apiserver**
- **Фронтенд кластера**: Предоставляет RESTful API Kubernetes, через который происходят все взаимодействия.
- **Валидация и настройка**: Проверяет и конфигурирует данные API-объектов.
- **Масштабируемость**: Поддерживает горизонтальное масштабирование, можно запускать несколько экземпляров.
- **Аутентификация и авторизация**: Интегрируется с системами аутентификации, проверяет права доступа.

### **2. etcd**
- **Хранилище состояния кластера**: Консистентное и высокодоступное key-value хранилище.
- **Хранит все данные**: Сохранение состояния всех объектов Kubernetes (Pods, Services, ConfigMaps и т.д.).
- **Основа консистентности**: Обеспечивает единое представление состояния кластера для всех компонентов.

### **3. kube-scheduler**
- **Распределение Pods по узлам**: Выбирает подходящий узел для запуска неподключенного Pod.
- **Учитывает критерии**:
  - Требования к ресурсам (CPU, память)
  - Ограничения аппаратного/программного обеспечения
  - Политики affinity/anti-affinity
  - Расположение данных и сроки выполнения

### **4. kube-controller-manager**
- **Запуск контроллеров**: Запускает основные процессы-контроллеры в едином бинарном файле:
  - **Node Controller**: Отслеживает состояние узлов, реагирует на их отказы
  - **Replication Controller**: Поддерживает правильное количество реплик для ReplicationController объектов
  - **Endpoints Controller**: Связывает Services и Pods через объекты Endpoints
  - **Service Account & Token Controllers**: Создает стандартные аккаунты и токены доступа API для новых namespace

### **5. cloud-controller-manager** (опционально)
- **Интеграция с облачными провайдерами**: Запускает контроллеры, специфичные для облачной платформы
- **Основные контроллеры**:
  - **Node Controller**: Проверяет состояние узлов в облаке
  - **Route Controller**: Настраивает маршруты в облачной инфраструктуре
  - **Service Controller**: Управляет облачными балансировщиками нагрузки
  - **Volume Controller**: Оркестрирует тома в облачном хранилище

## **Компоненты узлов (Node Components)**

### **6. kubelet**
- **Агент на узле**: Обеспечивает запуск контейнеров в Pods
- **Работает с PodSpecs**: Получает описание Pod и гарантирует, что указанные контейнеры работают
- **Мониторинг здоровья**: Отслеживает состояние контейнеров и перезапускает их при необходимости

### **7. kube-proxy**
- **Сетевой прокси**: Реализует часть концепции Service на каждом узле
- **Поддержка сетевых правил**: Управляет сетевыми соединениями к Pods
- **Режимы работы**: Использует iptables, ipvs или пользовательское пространство

### **8. Container Runtime**
- **Исполнение контейнеров**: Фундаментальный компонент для запуска контейнеров
- **Поддерживаемые среды**: containerd, CRI-O, Docker (через containerd)
- **Интерфейс CRI**: Работает через Container Runtime Interface

## **Дополнения (Addons)**

### **9. DNS**
- **Кластерный DNS**: Обязательный компонент для разрешения имен сервисов
- **Автоматическое обнаружение**: Контейнеры автоматически используют кластерный DNS

### **10. Dashboard** (опционально)
- **Веб-интерфейс**: Визуальное управление кластером и приложениями

### **11. Monitoring & Logging**
- **Сбор метрик**: Мониторинг ресурсов контейнеров (обычно через Heapster/Prometheus)
- **Логирование**: Централизованное хранение логов контейнеров

## **Взаимодействие компонентов для работы API**

1. **Запрос поступает в kube-apiserver**
   - API-сервер проверяет аутентификацию и авторизацию
   - Валидирует структуру запроса

2. **Сохранение состояния в etcd**
   - После валидации данные сохраняются в etcd
   - etcd гарантирует консистентность и репликацию данных

3. **Уведомление контроллеров**
   - Контроллеры наблюдают за изменениями через watch-механизм API
   - При изменении объектов в etcd контроллеры реагируют

4. **Планирование через kube-scheduler**
   - Для новых Pods scheduler выбирает подходящий узел
   - Обновляет информацию о Pod в etcd через API

5. **Исполнение на узлах**
   - Kubelet на узле получает информацию о назначенных ему Pods
   - Запускает контейнеры через container runtime
   - Kube-proxy обновляет сетевые правила для сервисов

6. **Обратная связь**
   - Kubelet обновляет статус Pod через API
   - Контроллеры отслеживают статус и вносят корректировки

## **Важные аспекты API сервера**

- **Версионирование API**: Поддержка нескольких версий API (/api/v1, /apis/extensions/v1beta1)
- **Группы API**: Организация API по группам (core, apps, networking.k8s.io)
- **OpenAPI спецификация**: Документация API доступна через /openapi/v2
- **Протоколы**: Поддержка JSON и Protobuf для сериализации
- **Масштабирование**: Возможность запуска нескольких экземпляров с балансировкой нагрузки

API-сервер является центральным компонентом, через который проходят все операции управления кластером, обеспечивая согласованное состояние системы через взаимодействие с etcd и другими компонентами управляющего слоя.


-----------


# Аннотации, Метки и Селекторы полей в Kubernetes

## **Аннотации (Annotations)**

### **Что это:**
- **Пользовательские метаданные**: Произвольные пары "ключ-значение" для добавления дополнительной информации к объектам Kubernetes.
- **Не используются для селекции**: В отличие от меток, не предназначены для фильтрации или выбора объектов.
- **Большой объем**: Могут содержать большие данные (до 256 КБ на ключ), включая структурированные и неструктурированные данные.

### **Назначение:**
1. **Хранение вспомогательной информации**:
   - Информация о сборке, выпуске (версия, хеш коммита, метка времени)
   - Контактные данные ответственных лиц
   - Ссылки на мониторинг, логирование, документацию
   - Конфигурационные параметры для инструментов и систем

2. **Отличие от меток**: Аннотации содержат информацию, которая:
   - Не используется для идентификации объектов
   - Может содержать символы, запрещенные в метках
   - Часто используется внешними инструментами и системами

### **Синтаксис:**
```yaml
metadata:
  annotations:
    key1: "value1"
    key2: "value2"
    example.com/build-info: "2024-01-15"
```

### **Примеры использования:**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: example-pod
  annotations:
    imageregistry: "https://hub.docker.com/"
    commit-hash: "a1b2c3d4"
    monitoring-url: "https://grafana.example.com/dashboard"
```

---

## **Метки (Labels)**

### **Что это:**
- **Идентификационные атрибуты**: Пары "ключ-значение", прикрепленные к объектам для их идентификации.
- **Используются для селекции**: Основной механизм группировки и выбора объектов в Kubernetes.
- **Ограниченный объем**: Предназначены для небольших, идентифицирующих данных.

### **Назначение:**
1. **Организация объектов**:
   - Группировка по окружению (dev, staging, production)
   - Классификация по версиям, компонентам, функциям
   - Управление жизненным циклом

2. **Селекция объектов**: Используются для:
   - Выбора Pods для Services
   - Управления ReplicaSets, Deployments
   - Определения политик сети, хранения

### **Синтаксис:**
```yaml
metadata:
  labels:
    app: "myapp"
    environment: "production"
    tier: "backend"
```

### **Типы селекторов меток:**

#### **1. Селекторы на равенстве:**
- **Операторы**: `=`, `==` (равно), `!=` (не равно)
- **Примеры**:
  ```bash
  # Выбор всех Pods с environment=production
  kubectl get pods -l environment=production
  
  # Выбор всех Pods с tier!=frontend
  kubectl get pods -l tier!=frontend
  ```

#### **2. Селекторы на множестве:**
- **Операторы**: `in`, `notin`, `exists`
- **Примеры**:
  ```bash
  # Выбор Pods с environment=production ИЛИ environment=staging
  kubectl get pods -l 'environment in (production, staging)'
  
  # Выбор Pods без метки tier
  kubectl get pods -l '!tier'
  
  # Выбор Pods с меткой environment, но не равной dev
  kubectl get pods -l 'environment,environment notin (dev)'
  ```

### **Пример использования в Service:**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: myapp
    tier: backend
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
```
Сервис будет направлять трафик ко всем Pods с метками `app=myapp` и `tier=backend`.

---

## **Селекторы полей (Field Selectors)**

### **Что это:**
- **Фильтрация по полям объектов**: Позволяет выбирать ресурсы на основе значений их полей.
- **Работают с любыми полями**: В отличие от меток, могут использовать любые поля объектов API.
- **Используются в основном в CLI**: Чаще всего применяются в командах `kubectl`.

### **Назначение:**
1. **Фильтрация по системным полям**:
   - Имя объекта (`metadata.name`)
   - Namespace (`metadata.namespace`)
   - Статус (`status.phase`)
   - Политики (`spec.restartPolicy`)

2. **Уточнение выборок**: Когда меток недостаточно для точного отбора.

### **Синтаксис:**
```bash
# Общий формат
kubectl get <ресурс> --field-selector <поле><оператор><значение>

# Примеры
kubectl get pods --field-selector status.phase=Running
kubectl get services --field-selector metadata.namespace!=default
```

### **Поддерживаемые операторы:**
- `=` или `==` (равно)
- `!=` (не равно)

### **Примеры использования:**

#### **1. Выбор по статусу:**
```bash
# Все работающие Pods
kubectl get pods --field-selector status.phase=Running

# Все Pods в состоянии Pending
kubectl get pods --field-selector status.phase=Pending
```

#### **2. Выбор по namespace:**
```bash
# Все объекты не в default namespace
kubectl get pods --all-namespaces --field-selector metadata.namespace!=default
```

#### **3. Составные селекторы:**
```bash
# Pods с restartPolicy=Always и статусом не Running
kubectl get pods --field-selector=spec.restartPolicy=Always,status.phase!=Running
```

#### **4. Выбор по имени:**
```bash
# Конкретный Pod по имени
kubectl get pods --field-selector metadata.name=my-pod
```

#### **5. Комбинация с селекторами меток:**
```bash
# Pods с меткой app=nginx и статусом Running
kubectl get pods -l app=nginx --field-selector status.phase=Running
```

### **Важные ограничения:**
- **Зависимость от типа ресурса**: Доступные поля для селекторов зависят от типа объекта.
- **Ошибка при несуществующем поле**: Если указать несуществующее поле - получите ошибку.
- **Не все поля поддерживаются**: Не все поля API доступны для field selectors.

### **Практический пример:**
```bash
# Найти все Pods, которые завершились с ошибкой (Failed)
kubectl get pods --field-selector status.phase=Failed

# Найти все Services вне default namespace
kubectl get services --all-namespaces --field-selector metadata.namespace!=default

# Найти все Pods с определенной политикой перезапуска
kubectl get pods --field-selector spec.restartPolicy=Never
```

---

## **Сравнительная таблица**

| Аспект | Метки | Аннотации | Селекторы полей |
|--------|-------|-----------|-----------------|
| **Назначение** | Идентификация и селекция | Хранение метаданных | Фильтрация по значениям полей |
| **Использование для выбора** | Да (основное) | Нет | Да (дополнительное) |
| **Объем данных** | Ограничен (идентификаторы) | Большой (до 256КБ) | Зависит от поля |
| **Синтаксис ключа** | Поддомен DNS + имя | Поддомен DNS + имя | Имя поля объекта |
| **Операторы** | `=`, `==`, `!=`, `in`, `notin`, `exists` | Нет | `=`, `==`, `!=` |
| **Пример использования** | Группировка Pods для Service | Хранение информации о сборке | Поиск объектов по статусу |
| **Типичные ключи** | `app`, `env`, `tier`, `version` | `build-info`, `contact`, `docs` | `status.phase`, `metadata.name` |

## **Ключевые выводы**

1. **Метки** - для организации и выбора объектов. Используйте для всего, что связано с логической группировкой.

2. **Аннотации** - для хранения дополнительной информации. Используйте для данных, которые нужны инструментам или людям, но не самому Kubernetes.

3. **Селекторы полей** - для фильтрации по системным атрибутам. Используйте, когда нужна выборка по состоянию или другим встроенным полям объектов.

4. **Комбинация подходов**: Часто используются вместе, например, выбираем по меткам, а затем фильтруем по полям.

-----------


# **Namespace в Kubernetes vs cgroups в Linux**

## **1. Namespace в Kubernetes**

### **Что это:**
- **Виртуальный кластер внутри физического**: Механизм изоляции ресурсов на логическом уровне в рамках одного кластера Kubernetes.
- **Область видимости для имен ресурсов**: Имена объектов должны быть уникальны только в пределах одного namespace.
- **Не является механизмом изоляции ресурсов**: Не ограничивает использование CPU, памяти, диска.

### **Основные характеристики:**
```yaml
# Создание namespace
apiVersion: v1
kind: Namespace
metadata:
  name: production
```

### **Для чего используется:**
1. **Мультитенантность**: Разделение кластера между командами, проектами, окружениями
2. **Ограничение доступа**: RBAC-политики применяются на уровне namespace
3. **Квоты ресурсов**: ResourceQuotas ограничивают использование ресурсов в namespace
4. **Сетевые политики**: NetworkPolicies могут ограничивать трафик между namespace

### **Типичные namespaces:**
```bash
# Предустановленные namespaces
kubectl get namespaces
# NAME          STATUS   AGE
# default       Active   1d      # По умолчанию для объектов
# kube-system   Active   1d      # Системные компоненты Kubernetes
# kube-public   Active   1d      # Публично доступные ресурсы
```

---

## **2. cgroups (control groups) в Linux**

### **Что это:**
- **Механизм ядра Linux**: Для ограничения, учета и изоляции использования системных ресурсов группами процессов.
- **Изоляция ресурсов**: CPU, память, дисковый I/O, сетевой трафик.
- **Уровень операционной системы**: Работает на уровне ядра, а не на уровне Kubernetes.

### **Основные контроллеры cgroups v2:**
```
cpu      - ограничение использования процессорного времени
memory   - ограничение использования памяти
io       - ограничение дискового ввода-вывода
pids     - ограничение количества процессов
cpuset   - привязка к конкретным CPU ядрам
```

### **Пример ограничения через cgroups:**
```bash
# Создание cgroup с ограничением памяти
mkdir /sys/fs/cgroup/memory/mygroup
echo 100M > /sys/fs/cgroup/memory/mygroup/memory.limit_in_bytes
echo $$ > /sys/fs/cgroup/memory/mygroup/cgroup.procs  # Добавление процесса
```

---

## **3. Различия и сходства**

### **Фундаментальные отличия:**

| Аспект | Kubernetes Namespace | Linux cgroups |
|--------|---------------------|---------------|
| **Уровень** | Уровень оркестрации (K8s) | Уровень ядра ОС |
| **Назначение** | Логическая изоляция и организация | Изоляция и ограничение ресурсов |
| **Что изолирует** | Имена ресурсов, политики доступа, сети | CPU, память, I/O, процессы |
| **Масштаб** | Весь кластер | Отдельный узел (нода) |
| **Пример** | Разделение dev/prod окружений | Ограничение памяти контейнера |

### **Как они взаимодействуют в Kubernetes:**

```
Kubernetes Namespace (логический)
    ↓
Pod (группа контейнеров)
    ↓
Container (процессы в контейнере)
    ↓
Linux cgroups (ограничение ресурсов для процессов)
    ↓
Ядро Linux
```

---

## **4. Как Kubernetes использует cgroups**

### **Спецификация ресурсов Pod:**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp
  namespace: production  # Kubernetes namespace
spec:
  containers:
  - name: app
    image: myapp:latest
    resources:
      limits:
        memory: "128Mi"   # → Преобразуется в cgroup memory.limit_in_bytes
        cpu: "500m"       # → Преобразуется в cgroup cpu.cfs_quota_us
      requests:
        memory: "64Mi"    # → Учитывается scheduler'ом
        cpu: "250m"       # → Учитывается scheduler'ом
```

### **Процесс работы:**
1. **Планирование** (kube-scheduler):
   - Проверяет `requests` для размещения Pod на узле с достаточными ресурсами
   - Namespace влияет через ResourceQuota (общие лимиты для namespace)

2. **Выполнение** (kubelet):
   - Создает cgroup для каждого контейнера
   - Настраивает лимиты через Container Runtime (containerd/docker)
   - Лимиты (`limits`) преобразуются в настройки cgroups

3. **Мониторинг**:
   - cgroups предоставляет метрики использования
   - kubelet собирает эти метрики через CRI

### **Иерархия cgroups в Kubernetes:**
```
/sys/fs/cgroup/kubepods.slice/
├── kubepods-burstable.slice/
│   ├── pod-<uid>.slice/
│   │   ├── container-1/     # cgroup контейнера 1
│   │   └── container-2/     # cgroup контейнера 2
│   └── pod-<another-uid>.slice/
└── kubepods-besteffort.slice/
```

---

## **5. Namespace и ResourceQuota**

### **Пример ResourceQuota в namespace:**
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: team-quota
  namespace: team-alpha  # Применяется только к этому namespace
spec:
  hard:
    requests.cpu: "10"        # Всего CPU requests в namespace
    requests.memory: 20Gi     # Всего memory requests
    limits.cpu: "20"          # Всего CPU limits
    limits.memory: 40Gi       # Всего memory limits
    pods: "50"                # Максимум Pods
    services: "10"            # Максимум Services
```

### **Как это работает вместе:**
1. **Пользователь создает Pod** в namespace `team-alpha`
2. **Kubernetes проверяет** ResourceQuota для namespace
3. **Если квота не превышена** → Pod создается
4. **Kubelet создает cgroup** для контейнера с указанными limits
5. **Ядро Linux обеспечивает** соблюдение лимитов через cgroups

---

## **6. Linux namespaces vs Kubernetes namespaces**

### **Важное различие!** Существуют также **Linux namespaces**, которые отличаются от Kubernetes namespaces:

| Тип | Linux Namespaces | Kubernetes Namespaces |
|-----|-----------------|----------------------|
| **PID namespace** | Изоляция идентификаторов процессов | Не связан |
| **Network namespace** | Изоляция сетевого стека (интерфейсы, порты) | Не связан напрямую |
| **Mount namespace** | Изоляция точек монтирования файловой системы | Не связан |
| **UTS namespace** | Изоляция hostname и domainname | Не связан |
| **IPC namespace** | Изоляция межпроцессного взаимодействия | Не связан |
| **User namespace** | Изоляция UID/GID пространства | Не связан |

### **Как контейнеры используют Linux namespaces:**
```bash
# Контейнер запускается с изоляцией через Linux namespaces
docker run -it --uts=host ubuntu bash  # Разделяет UTS namespace с хостом
```

---

## **7. Практический пример взаимодействия**

### **Сценарий:**
1. Команда разработки работает в namespace `dev`
2. Команда тестирования работает в namespace `qa`
3. Обе команды имеют разные ResourceQuotas
4. Каждый Pod имеет свои limits ресурсов

### **Архитектура:**
```
Kubernetes Cluster
├── Namespace: dev
│   ├── ResourceQuota: 20 CPU, 40GB RAM
│   ├── Pod: frontend (limits: 2 CPU, 4GB RAM)
│   │   └── cgroup: /kubepods/pod-xxx/container-frontend
│   └── Pod: backend (limits: 4 CPU, 8GB RAM)
│       └── cgroup: /kubepods/pod-yyy/container-backend
├── Namespace: qa
│   ├── ResourceQuota: 10 CPU, 20GB RAM
│   └── Pod: tester (limits: 1 CPU, 2GB RAM)
│       └── cgroup: /kubepods/pod-zzz/container-tester
└── Node (рабочий узел)
    ├── cgroup hierarchy
    └── Ядро Linux с cgroups
```

---

## **8. Ключевые выводы**

1. **Kubernetes namespaces** ≠ **Linux cgroups**:
   - Это совершенно разные механизмы на разных уровнях абстракции
   - Namespace — логическая группировка в Kubernetes
   - cgroups — механизм изоляции ресурсов в ядре Linux

2. **Взаимодействие**:
   - Kubernetes **использует** cgroups для реализации ограничений ресурсов
   - Namespace определяет **логические границы**, а cgroups — **физические ограничения**

3. **Linux namespaces** ≠ **Kubernetes namespaces**:
   - Это омонимы (одинаковые названия, разные понятия)
   - Linux namespaces обеспечивают изоляцию процессов, сети, файловой системы
   - Kubernetes namespaces обеспечивают логическую организацию ресурсов кластера

4. **Практическое применение**:
   - Используйте Kubernetes namespaces для разделения окружений, команд, проектов
   - Используйте ResourceQuotas для ограничения ресурсов в namespace
   - Limits в Pod спецификациях преобразуются в cgroups ограничения
   - cgroups обеспечивает соблюдение этих ограничений на уровне ОС

**Итог**: Kubernetes namespaces и Linux cgroups — это взаимодополняющие, но принципиально разные технологии, которые работают вместе для обеспечения как логической организации (namespace), так и физического ограничения ресурсов (cgroups).

-----------------
